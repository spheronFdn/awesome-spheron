version: "1.0"

services:
  vscode:
    image: lscr.io/linuxserver/code-server
    pull_policy: IfNotPresent
    expose:
      - port: 8443
        as: 8443
        to:
          - global: true
    env:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
      - PASSWORD=test
      - SUDO_PASSWORD=test
      - DEFAULT_WORKSPACE=/config/workspace
      - OLLAMA_HOST=ollama:11434
  ollama:
    image: ollama/ollama:latest
    pull_policy: IfNotPresent
    expose:
      - port: 11434
        as: 11434
        to:
          - global: false
          - service: vscode
    env:
      - OLLAMA_MODEL=llama3.2
    command:
      - "sh"
      - "-c"
      - "apt update && apt install -y curl && /bin/ollama serve & while ! curl -s http://localhost:11434/api/tags > /dev/null; do sleep 1; done && /bin/ollama pull $OLLAMA_MODEL && /bin/ollama run $OLLAMA_MODEL 'Hello' && tail -f /dev/null"
profiles:
  name: vscode-ollama
  duration: 1h
  mode: provider
  compute:
    vscode:
      resources:
        cpu:
          units: 4
        memory:
          size: 8Gi
        storage:
          - size: 10Gi
    ollama:
      resources:
        cpu:
          units: 8
        memory:
          size: 16Gi
        storage:
          - size: 200Gi
        gpu:
          units: 1
          attributes:
            vendor:
              nvidia:
                - model: rtx4090
  placement:
    westcoast:
      pricing:
        vscode:
          token: CST
          amount: 0.5
        ollama:
          token: CST
          amount: 1
deployment:
  vscode:
    westcoast:
      profile: vscode
      count: 1
  ollama:
    westcoast:
      profile: ollama
      count: 1
