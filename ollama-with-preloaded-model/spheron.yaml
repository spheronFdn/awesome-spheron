version: "1.0"

services:
  ollama:
    image: ollama/ollama:0.3.12
    pull_policy: IfNotPresent
    expose:
      - port: 11434
        as: 11434
        to:
          - global: true
    params:
      storage:
        default:
          mount: /mnt/data
          readOnly: false
    env:
      - OLLAMA_MODEL=llama3.2
    command:
      - "sh"
      - "-c"
      - "apt update && apt install -y curl && /bin/ollama serve & while ! curl -s http://localhost:11434/api/tags > /dev/null; do sleep 1; done && /bin/ollama pull $OLLAMA_MODEL && /bin/ollama run $OLLAMA_MODEL 'Hello' && tail -f /dev/null"
profiles:
  name: ollama
  duration: 1h
  mode: provider
  compute:
    ollama:
      resources:
        cpu:
          units: 4
        memory:
          size: 8Gi
        storage:
          name: "default"
          size: 100Gi
          attributes:
            persistent: true
            class: beta3
        gpu:
          units: 1
          attributes:
            vendor:
              nvidia:
                - model: rtx4090
  placement:
    westcoast:
      pricing:
        ollama:
          token: uSPON
          amount: 1

deployment:
  ollama:
    westcoast:
      profile: ollama
      count: 1
