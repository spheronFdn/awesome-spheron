version: "1.0"

services:
  ollama:
    image: ollama/ollama:0.3.12
    pull_policy: IfNotPresent
    expose:
      - port: 11434
        as: 11434
        to:
          - global: true
    env:
      - OLLAMA_MODEL=llama3.2
    command:
      - "sh"
      - "-c"
      - "apt update && apt install -y curl && /bin/ollama serve & while ! curl -s http://localhost:11434/api/tags > /dev/null; do sleep 1; done && /bin/ollama pull $OLLAMA_MODEL && /bin/ollama run $OLLAMA_MODEL 'Hello' && tail -f /dev/null"
profiles:
  name: ollama
  duration: 1h
  mode: provider
  compute:
    ollama:
      resources:
        cpu:
          units: 1
        memory:
          size: 2Gi
        storage:
          - size: 50Gi
        gpu:
          units: 1
          attributes:
            vendor:
              nvidia:
                - model: rtx4090
  placement:
    westcoast:
      pricing:
        ollama:
          token: uSPON
          amount: 1

deployment:
  ollama:
    westcoast:
      profile: ollama
      count: 1
